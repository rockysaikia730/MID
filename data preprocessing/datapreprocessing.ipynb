{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9829850",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d mehyarmlaweh/ner-annotated-cvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aab09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "dataset_name = \"ner-annotated-cvs.zip\"  \n",
    "with zipfile.ZipFile(dataset_name, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c41a233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import spacy\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "#import torch\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch,compounding\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec0765",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/Users/rocky/HSF/data preprocessing/dataset/ResumesJsonAnnotated/ResumesJsonAnnotated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ecfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(folder_path,num_of_files):\n",
    "    data = []\n",
    "    files = os.listdir(folder_path)\n",
    "    json_files =[file for file in files if file.endswith(\"json\")]\n",
    "    random.shuffle(json_files)\n",
    "    json_files = json_files[:num_of_files]\n",
    "    for filename in json_files:\n",
    "        json_file_path = os.path.join(folder_path, filename)\n",
    "        with open(json_file_path,\"r\") as file:\n",
    "            resume_data = json.load(file)\n",
    "            data.append(resume_data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d9ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = read_data(folder_path,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d867920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_overlapping(entities):\n",
    "    entities = sorted(entities,key = lambda x: (x[0],x[1]))\n",
    "    filtered_entities = []\n",
    "    last_end = -1\n",
    "    for start,end,label in entities:\n",
    "        if start >= last_end:\n",
    "            filtered_entities.append((start,end,label))\n",
    "            last_end = end\n",
    "    return filtered_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ae85c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "for data in all_data:\n",
    "    text = data[\"text\"].strip()\n",
    "    annotations = data[\"annotations\"]\n",
    "\n",
    "    entities = [(int(start), int(end), label) for start, end,label in annotations]\n",
    "    entities = filter_overlapping(entities)\n",
    "\n",
    "    training_data.append({\"text\":text,\"entities\":entities})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90558bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_entity(data):\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "    cleaned_data = []\n",
    "\n",
    "    for dic in data:\n",
    "        text = dic[\"text\"]\n",
    "        entities = dic[\"entities\"]\n",
    "        valid_entities = []\n",
    "        for start,end,label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "\n",
    "            while( valid_start < len(text) and invalid_span_tokens.match(text[valid_start])):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and valid_end <= len(text) and invalid_span_tokens.match(text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            if valid_start < valid_end:\n",
    "                valid_entities.append([valid_start,valid_end,label])\n",
    "            cleaned_data.append({\"text\":text,'entities': valid_entities})\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028aefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = clean_entity(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe325f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbbcb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import filter_spans\n",
    "\n",
    "for training_example in tqdm(training_data):\n",
    "    text = training_example[\"text\"]\n",
    "    labels = training_example[\"entities\"]\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "    for start,end,_ in labels:\n",
    "        span = doc.char_span(start,end,label=\"Skill\",alignment_mode=\"contract\")\n",
    "        if span is not None:\n",
    "            ents.append(span)\n",
    "\n",
    "    doc.ents = filter_spans(ents)\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "#doc_bin.to_disk(\"training_data.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f7c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from pathlib import Path\n",
    "\n",
    "def split_and_save_docbin(large_docbin, nlp, output_dir, docs_per_bin=20000):\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Get all docs from the large DocBin using the model's vocab\n",
    "    docs = list(large_docbin.get_docs(nlp.vocab))\n",
    "    total_docs = len(docs)\n",
    "    \n",
    "    saved_files = []\n",
    "    \n",
    "    # Split into smaller chunks\n",
    "    for i in range(0, total_docs, docs_per_bin):\n",
    "        # Create a new DocBin for this chunk\n",
    "        small_bin = DocBin()\n",
    "        \n",
    "        # Add docs to the smaller bin\n",
    "        chunk = docs[i:i + docs_per_bin]\n",
    "        for doc in chunk:\n",
    "            small_bin.add(doc)\n",
    "        \n",
    "        # Save this chunk\n",
    "        output_file = output_dir / f\"docs_{i//docs_per_bin}.spacy\"\n",
    "        small_bin.to_disk(output_file)\n",
    "        saved_files.append(output_file)\n",
    "        \n",
    "        #print(f\"Saved {len(chunk)} docs to {output_file}\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "\n",
    "output_directory = \"split_docs\"\n",
    "saved_files = split_and_save_docbin(doc_bin, nlp, output_directory)\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Load your spaCy model\n",
    "nlp = spacy.load(\"your_model\")  # e.g., \"en_core_web_sm\"\n",
    "\n",
    "# If you have your large DocBin in a variable called 'doc_bin':\n",
    "output_directory = \"split_docs\"\n",
    "saved_files = split_and_save_docbin(doc_bin, nlp, output_directory)\n",
    "\n",
    "# Later, to load all the docs:\n",
    "all_docs = []\n",
    "for file in Path(\"split_docs\").glob(\"*.spacy\"):\n",
    "    doc_bin = DocBin().from_disk(file)\n",
    "    all_docs.extend(list(doc_bin.get_docs(nlp.vocab)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28dc816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e62387",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9ac8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import spacy\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "from spacy.training.example import Example\n",
    "from spacy.util import minibatch,compounding\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b88c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35f3452",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc58630",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m spacy train config.cfg --output ./output --paths.train ./split_docs --paths.dev ./split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d54f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy train config.cfg --output ./output --paths.train ./split_docs/docs_0.spacy --paths.dev ./split_docs/docs_1.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b43c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0736d3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6731dbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367342ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01508b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ner.json\",\"r\") as f:\n",
    "    all_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129ecb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_overlapping(entities):\n",
    "    entities = sorted(entities,key = lambda x: (x[0],x[1]))\n",
    "    filtered_entities = []\n",
    "    last_end = -1\n",
    "    for start,end,label in entities:\n",
    "        if start >= last_end:\n",
    "            filtered_entities.append((start,end,label))\n",
    "            last_end = end\n",
    "    return filtered_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6c4e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "for data in all_data:\n",
    "    text = data[\"document\"].strip()\n",
    "    annotations = data[\"annotation\"]\n",
    "\n",
    "    entities = [(int(value['start']),int(value['end']), value['label']) for value in annotations]\n",
    "    entities = filter_overlapping(entities)\n",
    "\n",
    "    training_data.append({\"text\":text,\"entities\":entities})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e43d0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_entity(data):\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "    cleaned_data = []\n",
    "\n",
    "    for dic in data:\n",
    "        text = dic[\"text\"]\n",
    "        entities = dic[\"entities\"]\n",
    "        valid_entities = []\n",
    "        for start,end,label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "\n",
    "            while( valid_start < len(text) and invalid_span_tokens.match(text[valid_start])):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and valid_end <= len(text) and invalid_span_tokens.match(text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            if valid_start < valid_end:\n",
    "                valid_entities.append([valid_start,valid_end,label])\n",
    "            cleaned_data.append({\"text\":text,'entities': valid_entities})\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ab09ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = clean_entity(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e36299",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "\n",
    "n = len(training_data)\n",
    "train_size = int(0.8*n)\n",
    "train_data, val_data = random_split(training_data, [train_size,n-train_size ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1652f541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc_bin = DocBin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5430d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import filter_spans\n",
    "\n",
    "for training_example in tqdm(val_data):\n",
    "    text = training_example[\"text\"]\n",
    "    labels = training_example[\"entities\"]\n",
    "    doc = nlp.make_doc(text)\n",
    "    ents = []\n",
    "    for start,end,label in labels:\n",
    "        span = doc.char_span(start,end,label=label,alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            print(\"Skipping entity\")\n",
    "        else:\n",
    "            ents.append(span)\n",
    "\n",
    "    doc.ents = filter_spans(ents)\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "#doc_bin.to_disk(\"training_data.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb567dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bin.to_disk(\"val_data.spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e7df9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy init fill-config base_config.cfg config.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516854f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ Saving to output directory: output\u001b[0m\n",
      "\u001b[38;5;4mℹ Using CPU\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "python -m spacy train config.cfg --output ./output/ --paths.train ./train_data.spacy --paths.dev ./val_data.spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4e00320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "ner_model = spacy.load(\"output/model-best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed35d57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10+ Years ---> EXPERIENCE\n",
      "Business Analyst ---> SKILLS\n",
      "10+ years ---> EXPERIENCE\n",
      "BA ---> SKILLS\n",
      "banking ---> DOMAIN\n",
      "data mapping ---> SKILLS\n",
      "test ---> SKILLS\n",
      "JIRA ---> SKILLS\n",
      "Confluence ---> SKILLS\n",
      "management ---> SKILLS\n",
      "business ---> EDUCATION\n",
      "SQL ---> SKILLS\n",
      "Master Data Management (MDM ---> SKILLS\n",
      "ETL Tools ---> SKILLS\n",
      "ETL pipelines ---> SKILLS\n",
      "data integration ---> SKILLS\n",
      "JIRA ---> SKILLS\n",
      "Confluence ---> SKILLS\n",
      "Microsoft PowerPoint ---> SKILLS\n",
      "Stakeholder Management ---> SKILLS\n",
      "10+ years ---> EXPERIENCE\n",
      "Business Analysis ---> SKILLS\n",
      "Data Analysis ---> SKILLS\n",
      "banking ---> DOMAIN\n"
     ]
    }
   ],
   "source": [
    "job_description = \"\"\"\n",
    "Job Description – Senior Business Analyst / Data Analyst (10+ Years of Experience)\n",
    "\n",
    "\n",
    "\n",
    "We are seeking an experienced Business Analyst / Data Analyst with 10+ years of expertise in handling BA/DA roles, particularly in data-driven customer transformations. The ideal candidate will have strong analytical skills, technical proficiency, and experience working in the banking domain.\n",
    "\n",
    "\n",
    "\n",
    "Key Responsibilities:\n",
    "\n",
    "Define and agree on API contracts with consumers.\n",
    "Conduct data profiling and drive data definitions & data mapping.\n",
    "Document feed specifications and ensure alignment with business requirements.\n",
    "Write and refine user stories, capturing acceptance criteria and validating functional test scenarios in JIRA.\n",
    "Utilize Confluence for documentation and collaboration.\n",
    "Define and implement data quality measures, operational models, and exception management frameworks.\n",
    "Collaborate with external and internal stakeholders, ensuring smooth communication and project alignment.\n",
    "Leverage data-led customer transformation methodologies to drive business outcomes.\n",
    "\n",
    "\n",
    "Technical Skills:\n",
    "\n",
    "SQL & MongoDBI – Data extraction, transformation, and reporting.\n",
    "Postman – API testing and validation.\n",
    "Master Data Management (MDM) – Managing and governing enterprise data.\n",
    "ETL Tools – Experience in working with ETL pipelines and data integration.\n",
    "JIRA & Confluence – Agile project tracking and documentation.\n",
    "Microsoft PowerPoint & Stakeholder Management – Presenting insights effectively.\n",
    "\n",
    "\n",
    "Preferred Experience:\n",
    "\n",
    "10+ years of experience in Business Analysis / Data Analysis.\n",
    "Strong background in the banking sector.\n",
    "Experience in defining and managing data quality operations.\"\"\"\n",
    "\n",
    "# test the algorithm\n",
    "doc = ner_model(job_description)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, '--->', ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fdf513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datalytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
